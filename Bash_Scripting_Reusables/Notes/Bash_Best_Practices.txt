1. Use #!/bin/bash Shebang
Start your script with a shebang to specify the interpreter:
        #!/bin/bash


2. Set Strict Mode
Enable strict mode to catch errors early:
        set -euo pipefail

            -e: Exit immediately if a command exits with a non-zero status.
            -u: Treat unset variables as an error when substituting.
            -o pipefail: Return the exit status of the last command in a pipeline that failed.
            -x  # Enable debugging output

3. Use Meaningful Variable Names
Choose descriptive variable names to make your script self-documenting:
        input_file="data.txt"
        output_file="results.txt"


4. Quote Variables
Always quote your variables to prevent word splitting and globbing:

        if [ -f "$input_file" ]; then
            echo "File exists."
        fi

5. Use Functions:
Encapsulate code in functions to improve modularity and reuse:

        function process_file() {
            local file="$1"
            echo "Processing $file"
            # Further processing...
        }

        process_file "$input_file"

6. Handle Errors Gracefully
Check command exit statuses and handle errors appropriately:

        if ! cp "$input_file" "$output_file"; then
            echo "Error: Failed to copy file." >&2
            exit 1
        fi

7. Use Comments
Add comments to explain complex logic or important steps:

        # This function processes the input file and generates output.
        process_file() {
            # Implementation here...
        }

8. Use Arrays for Lists
Use arrays to manage collections of items, making the code cleaner:

        files=("file1.txt" "file2.txt" "file3.txt")

        for file in "${files[@]}"; do
            echo "Processing $file"
        done

9. Validate Inputs
Check user inputs and command-line arguments:

        if [ $# -ne 2 ]; then
            echo "Usage: $0 input_file output_file" >&2
            exit 1
        fi

10. Log Output
Consider logging output to a file for debugging or record-keeping:

        log_file="script.log"
        exec > >(tee -a "$log_file") 2>&1

Example:
        #!/bin/bash
        set -euo pipefail

        # Function to process files
        process_file() {
            local input_file="$1"
            local output_file="$2"
            echo "Processing $input_file and saving to $output_file"
            # Simulated processing
            cp "$input_file" "$output_file"
        }

        # Check for the correct number of arguments
        if [ $# -ne 2 ]; then
            echo "Usage: $0 input_file output_file" >&2
            exit 1
        fi

        input_file="$1"
        output_file="$2"

        # Validate input file
        if [ ! -f "$input_file" ]; then
            echo "Error: $input_file does not exist." >&2
            exit 1
        fi

        # Process the file
        process_file "$input_file" "$output_file" 

*******   >&2 and 2>/dev/null         

  >&2 and 2>/dev/null involve redirecting output, they serve different purposes. 

>&2
    Purpose: Redirects standard output (stdout) to standard error (stderr).
    Usage: Commonly used to send error messages to stderr so that they can be distinguished from normal output.
    Example:
        echo "Error: File not found." >&2

2>/dev/null
    Purpose: Redirects standard error (stderr) to /dev/null, effectively discarding any error messages.
    Usage: Used when you want to ignore error messages and prevent them from appearing in the terminal or log.
    Example:
        command 2>/dev/null

Key Differences:

    >&2 sends output to stderr, allowing it to be visible, whereas 2>/dev/null discards error messages.
    Use >&2 when you want to report an error to the user, and use 2>/dev/null when you want to suppress error messages.
    
Summary:
    Use >&2 for error reporting.
    Use 2>/dev/null when you want to ignore error messages.
        